{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3819f420",
   "metadata": {},
   "source": [
    "# Benchmark of existing approaches for detecting machine-generated text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd5a04",
   "metadata": {},
   "source": [
    "## Contents \n",
    "1. [GLTR](#GLRT)\n",
    "    1. [Install dependencies](#Install-dependencies)\n",
    "    1. [GLTR Code](#GLTR-Code)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa5fa6",
   "metadata": {},
   "source": [
    "## GLTR\n",
    "\n",
    "_Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. 2019. \"Gltr: Statistical detection and visualization of generated text\"._\n",
    "\n",
    "This is a zero-shot approach (no fine tuning of LM). The original code provides a webserver, which receives a text as input and returns a set of statistics, which are then rendered client-side. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d58d8",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "Dependencies from: [https://github.com/HendrikStrobelt/detecting-fake-text/blob/master/requirements.txt](detecting-fake-text/requirements.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c619d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.9/site-packages (4.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.9/site-packages (1.10.2)\n",
      "Requirement already satisfied: typing_extensions in /opt/anaconda3/lib/python3.9/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.9/site-packages (1.21.5)\n",
      "zsh:1: no matches found: connexion[swagger-ui]\n",
      "Requirement already satisfied: flask in /opt/anaconda3/lib/python3.9/site-packages (1.1.2)\n",
      "Requirement already satisfied: click>=5.1 in /opt/anaconda3/lib/python3.9/site-packages (from flask) (8.0.4)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/anaconda3/lib/python3.9/site-packages (from flask) (2.0.3)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/anaconda3/lib/python3.9/site-packages (from flask) (2.11.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/anaconda3/lib/python3.9/site-packages (from flask) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.9/site-packages (from Jinja2>=2.10.1->flask) (2.0.1)\n",
      "Requirement already satisfied: PyYaml in /opt/anaconda3/lib/python3.9/site-packages (6.0)\n",
      "Collecting flask_cors\n",
      "  Using cached Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: Flask>=0.9 in /opt/anaconda3/lib/python3.9/site-packages (from flask_cors) (1.1.2)\n",
      "Requirement already satisfied: Six in /opt/anaconda3/lib/python3.9/site-packages (from flask_cors) (1.16.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/anaconda3/lib/python3.9/site-packages (from Flask>=0.9->flask_cors) (2.11.3)\n",
      "Requirement already satisfied: click>=5.1 in /opt/anaconda3/lib/python3.9/site-packages (from Flask>=0.9->flask_cors) (8.0.4)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/anaconda3/lib/python3.9/site-packages (from Flask>=0.9->flask_cors) (2.0.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/anaconda3/lib/python3.9/site-packages (from Flask>=0.9->flask_cors) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.9/site-packages (from Jinja2>=2.10.1->Flask>=0.9->flask_cors) (2.0.1)\n",
      "Installing collected packages: flask_cors\n",
      "Successfully installed flask_cors-3.0.10\n"
     ]
    }
   ],
   "source": [
    "# pytorch-pretrained-bert>=0.6.1\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install connexion[swagger-ui]\n",
    "!pip install flask\n",
    "!pip install PyYaml\n",
    "!pip install flask_cors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a177608",
   "metadata": {},
   "source": [
    "### GLTR Code\n",
    "Source code: [https://github.com/HendrikStrobelt/detecting-fake-text](https://github.com/HendrikStrobelt/detecting-fake-text)\n",
    "\n",
    "Logic extracted form ´backend/api.py´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05840d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from transformers import (GPT2LMHeadModel, GPT2Tokenizer,\n",
    "                          BertTokenizer, BertForMaskedLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d01209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    values, _ = torch.topk(logits, k)\n",
    "    min_values = values[:, -1]\n",
    "    return torch.where(logits < min_values,\n",
    "                       torch.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "                       logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "101f2e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2BasedGLTR():\n",
    "    def __init__(self, model_name_or_path=\"gpt2\"):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.enc = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.start_token = self.enc(self.enc.bos_token, return_tensors='pt').data['input_ids'][0]\n",
    "        print(\"Loaded GPT-2 model!\")    \n",
    "\n",
    "    def check_probabilities(self, in_text, topk=40):\n",
    "        # Process input\n",
    "        token_ids = self.enc(in_text, return_tensors='pt').data['input_ids'][0]\n",
    "        token_ids = torch.concat([self.start_token, token_ids])\n",
    "        # Forward through the model\n",
    "        output = self.model(token_ids.to(self.device))\n",
    "        all_logits = output.logits[:-1].detach().squeeze()\n",
    "        # construct target and pred\n",
    "        # yhat = torch.softmax(logits[0, :-1], dim=-1)\n",
    "        all_probs = torch.softmax(all_logits, dim=1)\n",
    "\n",
    "        y = token_ids[1:]\n",
    "        # Sort the predictions for each timestep\n",
    "        sorted_preds = torch.argsort(all_probs, dim=1, descending=True).cpu()\n",
    "        # [(pos, prob), ...]\n",
    "        real_topk_pos = list(\n",
    "            [int(np.where(sorted_preds[i] == y[i].item())[0][0])\n",
    "             for i in range(y.shape[0])])\n",
    "        real_topk_probs = all_probs[np.arange(\n",
    "            0, y.shape[0], 1), y].data.cpu().numpy().tolist()\n",
    "        real_topk_probs = list(map(lambda x: round(x, 5), real_topk_probs))\n",
    "\n",
    "        real_topk = list(zip(real_topk_pos, real_topk_probs))\n",
    "        # [str, str, ...]\n",
    "        bpe_strings = self.enc.convert_ids_to_tokens(token_ids[:])\n",
    "\n",
    "        bpe_strings = [self.postprocess(s) for s in bpe_strings]\n",
    "\n",
    "        topk_prob_values, topk_prob_inds = torch.topk(all_probs, k=topk, dim=1)\n",
    "\n",
    "        pred_topk = [list(zip(self.enc.convert_ids_to_tokens(topk_prob_inds[i]),\n",
    "                              topk_prob_values[i].data.cpu().numpy().tolist()\n",
    "                              )) for i in range(y.shape[0])]\n",
    "        pred_topk = [[(self.postprocess(t[0]), t[1]) for t in pred] for pred in pred_topk]\n",
    "\n",
    "        # pred_topk = []\n",
    "        payload = {'bpe_strings': bpe_strings,\n",
    "                   'real_topk': real_topk,\n",
    "                   'pred_topk': pred_topk}\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return payload\n",
    "\n",
    "    def sample_unconditional(self, length=100, topk=5, temperature=1.0):\n",
    "        '''\n",
    "        Sample `length` words from the model.\n",
    "        Code strongly inspired by\n",
    "        https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_gpt2.py\n",
    "\n",
    "        '''\n",
    "        context = torch.full((1, 1),\n",
    "                             self.enc.encoder[self.start_token],\n",
    "                             device=self.device,\n",
    "                             dtype=torch.long)\n",
    "        prev = context\n",
    "        output = context\n",
    "        past = None\n",
    "        # Forward through the model\n",
    "        with torch.no_grad():\n",
    "            for i in range(length):\n",
    "                logits, past = self.model(prev, past=past)\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "                # Filter predictions to topk and softmax\n",
    "                probs = torch.softmax(top_k_logits(logits, k=topk),\n",
    "                                      dim=-1)\n",
    "                # Sample\n",
    "                prev = torch.multinomial(probs, num_samples=1)\n",
    "                # Construct output\n",
    "                output = torch.cat((output, prev), dim=1)\n",
    "\n",
    "        output_text = self.enc.decode(output[0].tolist())\n",
    "        return output_text\n",
    "\n",
    "    def postprocess(self, token):\n",
    "        with_space = False\n",
    "        with_break = False\n",
    "        if token.startswith('Ġ'):\n",
    "            with_space = True\n",
    "            token = token[1:]\n",
    "            # print(token)\n",
    "        elif token.startswith('â'):\n",
    "            token = ' '\n",
    "        elif token.startswith('Ċ'):\n",
    "            token = ' '\n",
    "            with_break = True\n",
    "\n",
    "        token = '-' if token.startswith('â') else token\n",
    "        token = '“' if token.startswith('ľ') else token\n",
    "        token = '”' if token.startswith('Ŀ') else token\n",
    "        token = \"'\" if token.startswith('Ļ') else token\n",
    "\n",
    "        if with_space:\n",
    "            token = '\\u0120' + token\n",
    "        if with_break:\n",
    "            token = '\\u010A' + token\n",
    "\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "311407ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTBasedGLTR():\n",
    "    def __init__(self, model_name_or_path=\"bert-base-cased\"):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            do_lower_case=False)\n",
    "        self.model = BertForMaskedLM.from_pretrained(\n",
    "            model_name_or_path)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        # BERT-specific symbols\n",
    "        self.mask_tok = self.tokenizer.convert_tokens_to_ids([\"[MASK]\"])[0]\n",
    "        self.pad = self.tokenizer.convert_tokens_to_ids([\"[PAD]\"])[0]\n",
    "        print(\"Loaded BERT model!\")\n",
    "\n",
    "    def check_probabilities(self, in_text, topk=40, max_context=20,\n",
    "                            batch_size=20):\n",
    "        '''\n",
    "        Same behavior as GPT-2\n",
    "        Extra param: max_context controls how many words should be\n",
    "        fed in left and right\n",
    "        Speeds up inference since BERT requires prediction word by word\n",
    "        '''\n",
    "        in_text = \"[CLS] \" + in_text + \" [SEP]\"\n",
    "        tokenized_text = self.tokenizer.tokenize(in_text)\n",
    "        # Construct target\n",
    "        y_toks = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        # Only use sentence A embedding here since we have non-separable seq's\n",
    "        segments_ids = [0] * len(y_toks)\n",
    "        y = torch.tensor([y_toks]).to(self.device)\n",
    "        segments_tensor = torch.tensor([segments_ids]).to(self.device)\n",
    "\n",
    "        # TODO batching...\n",
    "        # Create batches of (x,y)\n",
    "        input_batches = []\n",
    "        target_batches = []\n",
    "        for min_ix in range(0, len(y_toks), batch_size):\n",
    "            max_ix = min(min_ix + batch_size, len(y_toks) - 1)\n",
    "            cur_input_batch = []\n",
    "            cur_target_batch = []\n",
    "            # Construct each batch\n",
    "            for running_ix in range(max_ix - min_ix):\n",
    "                tokens_tensor = y.clone()\n",
    "                mask_index = min_ix + running_ix\n",
    "                tokens_tensor[0, mask_index + 1] = self.mask_tok\n",
    "\n",
    "                # Reduce computational complexity by subsetting\n",
    "                min_index = max(0, mask_index - max_context)\n",
    "                max_index = min(tokens_tensor.shape[1] - 1,\n",
    "                                mask_index + max_context + 1)\n",
    "\n",
    "                tokens_tensor = tokens_tensor[:, min_index:max_index]\n",
    "                # Add padding\n",
    "                needed_padding = max_context * 2 + 1 - tokens_tensor.shape[1]\n",
    "                if min_index == 0 and max_index == y.shape[1] - 1:\n",
    "                    # Only when input is shorter than max_context\n",
    "                    left_needed = (max_context) - mask_index\n",
    "                    right_needed = needed_padding - left_needed\n",
    "                    p = torch.nn.ConstantPad1d((left_needed, right_needed),\n",
    "                                               self.pad)\n",
    "                    tokens_tensor = p(tokens_tensor)\n",
    "                elif min_index == 0:\n",
    "                    p = torch.nn.ConstantPad1d((needed_padding, 0), self.pad)\n",
    "                    tokens_tensor = p(tokens_tensor)\n",
    "                elif max_index == y.shape[1] - 1:\n",
    "                    p = torch.nn.ConstantPad1d((0, needed_padding), self.pad)\n",
    "                    tokens_tensor = p(tokens_tensor)\n",
    "\n",
    "                cur_input_batch.append(tokens_tensor)\n",
    "                cur_target_batch.append(y[:, mask_index + 1])\n",
    "                # new_segments = segments_tensor[:, min_index:max_index]\n",
    "            cur_input_batch = torch.cat(cur_input_batch, dim=0)\n",
    "            cur_target_batch = torch.cat(cur_target_batch, dim=0)\n",
    "            input_batches.append(cur_input_batch)\n",
    "            target_batches.append(cur_target_batch)\n",
    "\n",
    "        real_topk = []\n",
    "        pred_topk = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in zip(input_batches, target_batches):\n",
    "                # Compute one batch of inputs\n",
    "                # By construction, MASK is always the middle\n",
    "                logits = self.model(src, torch.zeros_like(src))[:,\n",
    "                         max_context + 1]\n",
    "                yhat = torch.softmax(logits, dim=-1)\n",
    "\n",
    "                sorted_preds = np.argsort(-yhat.data.cpu().numpy())\n",
    "                # TODO: compare with batch of tgt\n",
    "\n",
    "                # [(pos, prob), ...]\n",
    "                real_topk_pos = list(\n",
    "                    [int(np.where(sorted_preds[i] == tgt[i].item())[0][0])\n",
    "                     for i in range(yhat.shape[0])])\n",
    "                real_topk_probs = yhat[np.arange(\n",
    "                    0, yhat.shape[0], 1), tgt].data.cpu().numpy().tolist()\n",
    "                real_topk.extend(list(zip(real_topk_pos, real_topk_probs)))\n",
    "\n",
    "                # # [[(pos, prob), ...], [(pos, prob), ..], ...]\n",
    "                pred_topk.extend([list(zip(self.tokenizer.convert_ids_to_tokens(\n",
    "                    sorted_preds[i][:topk]),\n",
    "                    yhat[i][sorted_preds[i][\n",
    "                            :topk]].data.cpu().numpy().tolist()))\n",
    "                    for i in range(yhat.shape[0])])\n",
    "\n",
    "        bpe_strings = [self.postprocess(s) for s in tokenized_text]\n",
    "        pred_topk = [[(self.postprocess(t[0]), t[1]) for t in pred] for pred in pred_topk]\n",
    "        payload = {'bpe_strings': bpe_strings,\n",
    "                   'real_topk': real_topk,\n",
    "                   'pred_topk': pred_topk}\n",
    "        return payload\n",
    "\n",
    "    def postprocess(self, token):\n",
    "\n",
    "        with_space = True\n",
    "        with_break = token == '[SEP]'\n",
    "        if token.startswith('##'):\n",
    "            with_space = False\n",
    "            token = token[2:]\n",
    "\n",
    "        if with_space:\n",
    "            token = '\\u0120' + token\n",
    "        if with_break:\n",
    "            token = '\\u010A' + token\n",
    "        #\n",
    "        # # print ('....', token)\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ab7120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bert_prob(raw_text):\n",
    "    '''\n",
    "    Tests for BERT\n",
    "    '''\n",
    "    lm = BERTBasedGLTR()\n",
    "    start = time.time()\n",
    "    payload = lm.check_probabilities(raw_text, topk=5)\n",
    "    end = time.time()\n",
    "    print(\"{:.2f} Seconds for a run with BERT\".format(end - start))\n",
    "    print(f\"Payload: {payload}\")\n",
    "\n",
    "def test_gpt2_prob(raw_text):\n",
    "    '''\n",
    "    Tests for GPT-2\n",
    "    '''\n",
    "    lm = GPT2BasedGLTR()\n",
    "    start = time.time()\n",
    "    payload = lm.check_probabilities(raw_text, topk=5)\n",
    "    end = time.time()\n",
    "    print(\"{:.2f} Seconds for a check with GPT-2\".format(end - start))\n",
    "    print(f\"payload: {payload}\")\n",
    "\n",
    "\n",
    "def test_gpt2_unconditional_sampling():\n",
    "    '''\n",
    "    Tests for GPT-2\n",
    "    '''\n",
    "    lm = GPT2BasedGLTR()\n",
    "    start = time.time()\n",
    "    sample = lm.sample_unconditional()\n",
    "    end = time.time()\n",
    "    print(\"{:.2f} Seconds for a sample from GPT-2\".format(end - start))\n",
    "    print(\"SAMPLE:\", sample)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f269b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"\n",
    "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
    "\n",
    "The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.\n",
    "\n",
    "Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.\n",
    "\n",
    "Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow.\n",
    "\n",
    "Pérez and the others then ventured further into the valley. “By the time we reached the top of one peak, the water looked blue, with some crystals on top,” said Pérez.\n",
    "\n",
    "Pérez and his friends were astonished to see the unicorn herd. These creatures could be seen from the air without having to move too much to see them – they were so close they could touch their horns.\n",
    "\n",
    "While examining these bizarre creatures the scientists discovered that the creatures also spoke some fairly regular English. Pérez stated, “We can see, for example, that they have a common ‘language,’ something like a dialect or dialectic.”\n",
    "\n",
    "Dr. Pérez believes that the unicorns may have originated in Argentina, where the animals were believed to be descendants of a lost race of people who lived there before the arrival of humans in those parts of South America.\n",
    "\n",
    "While their origins are still unclear, some believe that perhaps the creatures were created when a human and a unicorn met each other in a time before human civilization. According to Pérez, “In South America, such incidents seem to be quite common.”\n",
    "\n",
    "However, Pérez also pointed out that it is likely that the only way of knowing for sure if unicorns are indeed the descendants of a lost alien race is through DNA. “But they seem to be able to communicate in English quite well, which I believe is a sign of evolution, or at least a change in social organization,” said the scientist.\n",
    "\"\"\"\n",
    "raw_text = \"\"\"\n",
    "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b39432e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT-2 model!\n",
      "0.44 Seconds for a check with GPT-2\n",
      "payload: {'bpe_strings': ['<|endoftext|>', 'Ċ ', 'In', 'Ġa', 'Ġshocking', 'Ġfinding', ',', 'Ġscientist', 'Ġdiscovered', 'Ġa', 'Ġherd', 'Ġof', 'Ġunic', 'orns', 'Ġliving', 'Ġin', 'Ġa', 'Ġremote', ',', 'Ġpreviously', 'Ġunexpl', 'ored', 'Ġvalley', ',', 'Ġin', 'Ġthe', 'ĠAnd', 'es', 'ĠMountains', '.', 'ĠEven', 'Ġmore', 'Ġsurprising', 'Ġto', 'Ġthe', 'Ġresearchers', 'Ġwas', 'Ġthe', 'Ġfact', 'Ġthat', 'Ġthe', 'Ġunic', 'orns', 'Ġspoke', 'Ġperfect', 'ĠEnglish', '.', 'Ċ '], 'real_topk': [(0, 0.0623), (2, 0.02149), (1, 0.09866), (16, 0.00723), (82, 0.00113), (0, 0.48223), (1516, 4e-05), (82, 0.00165), (1, 0.10015), (2767, 2e-05), (0, 0.97461), (868, 0.00016), (0, 0.99857), (11, 0.01713), (0, 0.54262), (0, 0.34731), (7, 0.01207), (5, 0.04114), (417, 0.00027), (2, 0.0744), (0, 0.99256), (8, 0.01505), (4, 0.04378), (1, 0.06449), (0, 0.17177), (68, 0.00269), (0, 0.56245), (1, 0.12027), (0, 0.3117), (68, 0.0006), (1, 0.09908), (2, 0.09152), (6, 0.00971), (1, 0.12462), (0, 0.07961), (1, 0.26692), (1, 0.35177), (0, 0.52609), (0, 0.92775), (0, 0.41226), (0, 0.17672), (0, 0.99786), (181, 0.00036), (170, 0.00055), (0, 0.59237), (0, 0.46648), (0, 0.67021)], 'pred_topk': [[('Ċ ', 0.06229906529188156), ('The', 0.0376996174454689), ('\"', 0.02411317080259323), ('A', 0.019402991980314255), ('I', 0.018320975825190544)], [('The', 0.08049722760915756), ('A', 0.04129629582166672), ('In', 0.02148725837469101), ('\"', 0.02127162553369999), ('This', 0.017059531062841415)], [('Ġthe', 0.15741321444511414), ('Ġa', 0.09866011142730713), ('Ġthis', 0.07084135711193085), ('Ġan', 0.03587491065263748), ('Ġhis', 0.023675713688135147)], [('Ġrecent', 0.04635023698210716), ('Ġnew', 0.034102290868759155), ('Ġstatement', 0.031161315739154816), ('Ġmove', 0.029254592955112457), ('Ġrare', 0.01812831684947014)], [('Ġtwist', 0.08172599226236343), ('Ġdevelopment', 0.07948093861341476), ('Ġmove', 0.06181628257036209), ('Ġnew', 0.045100681483745575), ('Ġincident', 0.035903118550777435)], [(',', 0.4822322726249695), ('Ġthat', 0.11817609518766403), ('Ġin', 0.05890876427292824), ('Ġfrom', 0.026671744883060455), ('Ġon', 0.024245744571089745)], [('Ġa', 0.22065287828445435), ('Ġthe', 0.19109323620796204), ('Ġan', 0.03025887720286846), ('Ġpolice', 0.02518521063029766), ('Ġtwo', 0.019150543957948685)], [('ĠDr', 0.08545631915330887), ('Ġat', 0.033410072326660156), ('Ġwho', 0.032884567975997925), ('Ġand', 0.02722381427884102), ('ĠDavid', 0.02353581227362156)], [('Ġthat', 0.5031290054321289), ('Ġa', 0.10015342384576797), ('Ġthe', 0.07146347314119339), ('Ġan', 0.016897182911634445), ('Ġin', 0.008413643576204777)], [('Ġnew', 0.13334792852401733), ('Ġway', 0.020233849063515663), ('Ġmysterious', 0.01722683385014534), ('Ġsecret', 0.014007452875375748), ('Ġrare', 0.013953162357211113)], [('Ġof', 0.9746137857437134), ('Ġthat', 0.00411733565852046), ('-', 0.0020049000158905983), ('Ġin', 0.001077600521966815), ('Ġanimal', 0.0007665974553674459)], [('Ġwolves', 0.020470794290304184), ('Ġsheep', 0.017612360417842865), ('Ġcows', 0.01657029427587986), ('Ġwild', 0.015279252082109451), ('Ġelephants', 0.015270978212356567)], [('orns', 0.9985707998275757), ('urs', 0.0003584903897717595), ('orn', 0.0003571717825252563), ('am', 0.00015656267351005226), ('ast', 0.0001403520436724648)], [('Ġthat', 0.10292388498783112), ('Ġin', 0.0853164866566658), ('Ġwere', 0.04446638375520706), (',', 0.03531698137521744), ('Ġhad', 0.026172565296292305)], [('Ġin', 0.5426182746887207), ('Ġon', 0.20339997112751007), ('Ġnear', 0.01936691254377365), ('Ġat', 0.019099123775959015), ('Ġunder', 0.01768200471997261)], [('Ġa', 0.34730929136276245), ('Ġthe', 0.1866431087255478), ('Ġan', 0.06568505614995956), ('Ġtheir', 0.0150831900537014), ('Ġone', 0.00979574304074049)], [('Ġcave', 0.05522030219435692), ('Ġforest', 0.039681851863861084), ('Ġsmall', 0.02483435347676277), ('Ġdesert', 0.02223862335085869), ('Ġtiny', 0.01978578045964241)], [('Ġarea', 0.08822474628686905), ('Ġpart', 0.07736364752054214), ('Ġforest', 0.04389756917953491), ('Ġvillage', 0.04353436082601547), ('Ġmountain', 0.04131605103611946)], [('Ġremote', 0.08557933568954468), ('Ġbarren', 0.044719573110342026), ('Ġisolated', 0.04429207742214203), ('Ġmountainous', 0.0383298434317112), ('Ġdesert', 0.026989616453647614)], [('Ġunknown', 0.20462247729301453), ('Ġund', 0.07677866518497467), ('Ġunexpl', 0.07439883798360825), ('Ġuntouched', 0.0714145079255104), ('Ġunin', 0.06730971485376358)], [('ored', 0.992563784122467), ('o', 0.005456667393445969), ('oded', 0.0017635643016546965), ('oted', 3.350843326188624e-05), ('oring', 2.8896214644191787e-05)], [('Ġarea', 0.235428124666214), ('Ġregion', 0.08906712383031845), ('Ġpart', 0.057222019881010056), ('Ġworld', 0.047754283994436264), (',', 0.02104598842561245)], [('Ġin', 0.24121853709220886), ('.', 0.16547226905822754), ('Ġof', 0.08799973875284195), ('Ġnear', 0.05516975373029709), (',', 0.04377659782767296)], [('Ġand', 0.09624892473220825), ('Ġin', 0.06449435651302338), ('Ġwhich', 0.0374399833381176), ('Ġthat', 0.03038373403251171), ('Ġwhere', 0.023953134194016457)], [('Ġthe', 0.1717691719532013), ('Ġa', 0.08734775334596634), ('Ġan', 0.03854799643158913), ('Ġwhat', 0.02815093845129013), ('Ġnorthern', 0.02447793260216713)], [('ĠHimal', 0.0707312822341919), ('Ġmiddle', 0.036867205053567886), ('Ġearly', 0.01976734586060047), ('ĠSouth', 0.01679079793393612), ('Ġwild', 0.014945442788302898)], [('es', 0.5624505281448364), ('aman', 0.32369616627693176), ('ean', 0.06460169702768326), ('hra', 0.01244560070335865), ('am', 0.005189879797399044)], [('.', 0.33858412504196167), ('ĠMountains', 0.12027357518672943), (',', 0.11130447685718536), ('Ġmountains', 0.03984186053276062), ('Ġin', 0.03134676441550255)], [('.', 0.31169527769088745), ('Ġof', 0.2705412209033966), (',', 0.1378188580274582), ('Ġin', 0.06050792336463928), ('Ġnear', 0.021978145465254784)], [('Ċ ', 0.5705966949462891), ('ĠThe', 0.11535821110010147), ('ĠThey', 0.039254944771528244), ('Ċ ', 0.02363431826233864), ('ĠIt', 0.013961266726255417)], [('Ġthough', 0.4125312268733978), ('Ġmore', 0.09908229857683182), ('Ġafter', 0.05376049131155014), ('Ġif', 0.044675566256046295), ('Ġthe', 0.042657703161239624)], [('Ġshocking', 0.4690394401550293), ('Ġdisturbing', 0.11445999890565872), ('Ġsurprising', 0.09151832014322281), ('Ġstartling', 0.05409678444266319), ('Ġalarming', 0.03843360021710396)], [(',', 0.6054142713546753), ('Ġwas', 0.14690929651260376), ('Ġis', 0.1092839241027832), ('Ġthan', 0.03721391037106514), (':', 0.018347565084695816)], [('Ġscientists', 0.15192733705043793), ('Ġthe', 0.12461593002080917), ('Ġus', 0.06157650426030159), ('Ġresearchers', 0.05710994452238083), ('Ġme', 0.04694654420018196)], [('Ġresearchers', 0.07960686087608337), ('Ġlocals', 0.07864820212125778), ('Ġscientists', 0.05635204166173935), ('Ġworld', 0.055769503116607666), ('Ġpublic', 0.033728502690792084)], [(',', 0.35914385318756104), ('Ġwas', 0.2669218182563782), ('Ġis', 0.20990251004695892), ('Ġwere', 0.05550830438733101), ('Ġare', 0.01827114075422287)], [('Ġthat', 0.36623555421829224), ('Ġthe', 0.35176563262939453), ('Ġhow', 0.11521808058023453), ('Ġtheir', 0.05904325842857361), ('Ġa', 0.02719244547188282)], [('Ġfact', 0.5260913968086243), ('Ġdiscovery', 0.06952818483114243), ('Ġfinding', 0.024356279522180557), ('Ġpresence', 0.02406132034957409), ('Ġlocation', 0.01366263534873724)], [('Ġthat', 0.9277462959289551), ('Ġthe', 0.023569297045469284), ('Ġthey', 0.019715284928679466), (',', 0.006437738426029682), ('Ġof', 0.0022414682898670435)], [('Ġthe', 0.41226404905319214), ('Ġthey', 0.1753404289484024), ('Ġthese', 0.04886630177497864), ('Ġthis', 0.03487551212310791), ('Ġtheir', 0.031698841601610184)], [('Ġunic', 0.17671675980091095), ('Ġanimals', 0.1306043416261673), ('Ġunicorn', 0.08987602591514587), ('Ġherd', 0.03799816966056824), ('Ġcreatures', 0.021768301725387573)], [('orns', 0.9978581070899963), ('orn', 0.001868563238531351), ('urs', 0.00010835757711902261), ('opes', 1.7937041775439866e-05), ('ur', 9.246591616829392e-06)], [('Ġwere', 0.2788822054862976), ('Ġhad', 0.09003075957298279), ('Ġare', 0.04475006088614464), (\"'\", 0.03777659684419632), (',', 0.03506287559866905)], [('Ġa', 0.1143052726984024), ('Ġin', 0.06802916526794434), ('Ġthe', 0.05839790403842926), ('ĠEnglish', 0.04698314145207405), ('Ġonly', 0.041689906269311905)], [('ĠEnglish', 0.5923733711242676), ('ĠSpanish', 0.08719278126955032), ('ĠItalian', 0.022191081196069717), ('ĠFrench', 0.020802421495318413), (',', 0.014082281850278378)], [('.', 0.4664754867553711), (',', 0.25778132677078247), ('Ġand', 0.0801902711391449), (' ', 0.01763886585831642), ('Ġ-', 0.011715107597410679)], [('Ċ ', 0.6702072620391846), ('ĠThe', 0.06071028113365173), ('Ċ ', 0.022994374856352806), ('ĠThey', 0.01900622434914112), ('Ġ\"', 0.01618041656911373)]]}\n"
     ]
    }
   ],
   "source": [
    "test_gpt2_prob(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3386c11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT model!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q_/l2hs4hc95knggbj50_3pk_tw0000gn/T/ipykernel_22019/2700344311.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_bert_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/q_/l2hs4hc95knggbj50_3pk_tw0000gn/T/ipykernel_22019/2507444829.py\u001b[0m in \u001b[0;36mtest_bert_prob\u001b[0;34m(raw_text)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTBasedGLTR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{:.2f} Seconds for a run with BERT\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/q_/l2hs4hc95knggbj50_3pk_tw0000gn/T/ipykernel_22019/2204163533.py\u001b[0m in \u001b[0;36mcheck_probabilities\u001b[0;34m(self, in_text, topk, max_context, batch_size)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;31m# Compute one batch of inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;31m# By construction, MASK is always the middle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 logits = self.model(src, torch.zeros_like(src))[:,\n\u001b[0m\u001b[1;32m     86\u001b[0m                          max_context + 1]\n\u001b[1;32m     87\u001b[0m                 \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minner_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "test_bert_prob(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6065a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GPT-2 model!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "tensor([50256])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q_/l2hs4hc95knggbj50_3pk_tw0000gn/T/ipykernel_22019/76972821.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_gpt2_unconditional_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/q_/l2hs4hc95knggbj50_3pk_tw0000gn/T/ipykernel_22019/2507444829.py\u001b[0m in \u001b[0;36mtest_gpt2_unconditional_sampling\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2BasedGLTR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_unconditional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{:.2f} Seconds for a sample from GPT-2\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/q_/l2hs4hc95knggbj50_3pk_tw0000gn/T/ipykernel_22019/72605430.py\u001b[0m in \u001b[0;36msample_unconditional\u001b[0;34m(self, length, topk, temperature)\u001b[0m\n\u001b[1;32m     61\u001b[0m         '''\n\u001b[1;32m     62\u001b[0m         context = torch.full((1, 1),\n\u001b[0;32m---> 63\u001b[0;31m                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                              \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                              dtype=torch.long)\n",
      "\u001b[0;31mKeyError\u001b[0m: tensor([50256])"
     ]
    }
   ],
   "source": [
    "test_gpt2_unconditional_sampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c333a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
