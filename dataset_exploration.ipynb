{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c005f485",
   "metadata": {},
   "source": [
    "# Dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25dd3dc",
   "metadata": {},
   "source": [
    "This notebook simplifies the analysis and exploration of the datasets used for the human/machine classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2c239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download datasets\n",
    "%run 'download_dataset.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data\"\n",
    "datasets = sorted([f for f in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, f)) and f[0] != \".\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9297afd",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d0788f",
   "metadata": {},
   "source": [
    "Tokenization and filtering function to preprocess datasets (removing digits-only tokens and non-english symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0a425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing as pp\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def tokenize_corpus(corpus):\n",
    "    tokenized_corpus = make_pipeline(\n",
    "        pp.WordTokenizer(), \n",
    "        pp.WordsFilter(drop_symbols=True, drop_digits=True)\n",
    "    ).fit_transform(corpus)\n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f0fb48",
   "metadata": {},
   "source": [
    "### Vocabulary extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98792c41",
   "metadata": {},
   "source": [
    "Get vocabulary from corpus using _CountVectorizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def get_features(corpus, min_df=1):\n",
    "    vectorizer = CountVectorizer(min_df=min_df, preprocessor=identity, tokenizer=identity)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return set(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03de3d0",
   "metadata": {},
   "source": [
    "## Dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5135f3fa",
   "metadata": {},
   "source": [
    "### Word count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ac68a2",
   "metadata": {},
   "source": [
    "Get min/max/avg tokens per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15bf418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset, data_path=\"./data\"):\n",
    "    df = None\n",
    "    path = os.path.join(data_path, dataset)\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_json(path, lines = True)\n",
    "        # adjust DataFrame based on original data structure\n",
    "        if len(df.columns)==1:\n",
    "            # json contains only texts (GPT3 samples)\n",
    "            df.rename(columns={0: 'text'}, inplace=True)\n",
    "        elif 'article' in df.columns:\n",
    "            # Grover datasets\n",
    "            df.rename(columns={'article': 'text'}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3629099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = {}\n",
    "stats = {}\n",
    "for i, dataset in enumerate(datasets):\n",
    "    df = load_dataset(dataset, data_path)\n",
    "    if df is not None:\n",
    "        ds_name = os.path.splitext(dataset)[0]\n",
    "        \n",
    "        # store tokens for later analysis\n",
    "        dataset_tokens[ds_name] = tokenize_corpus(list(df['text']))\n",
    "        df['length'] = [len(doc) for doc in dataset_tokens[ds_name]]\n",
    "        \n",
    "        description = df[['text', 'length']].describe()\n",
    "        n_entries = description.loc['count']['length']\n",
    "        max_tokens = int(description.loc['max']['length'])\n",
    "        min_tokens = int(description.loc['min']['length'])\n",
    "        avg_tokens = description.loc['mean']['length']\n",
    "        \n",
    "        stats[i] = [ds_name, n_entries, max_tokens, min_tokens, avg_tokens]\n",
    "\n",
    "df_stats = pd.DataFrame.from_dict(stats, \n",
    "                                  orient='index', \n",
    "                                  columns=[\"source\", \"n_entries\", \"max_tokens\", \"min_tokens\", \"avg_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccd916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea7af0",
   "metadata": {},
   "source": [
    "### Vocabulary analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3012703f",
   "metadata": {},
   "source": [
    "Extract vocabulary from larger datasets flagged as \"training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf8a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_datasets = [x for x in datasets if \"train\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a27b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf4b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_features = {}\n",
    "for dataset in selected_datasets:\n",
    "    ds = os.path.splitext(dataset)[0]\n",
    "    # check if matching tokenized dataset already present\n",
    "    if ds not in dataset_tokens:\n",
    "        df = load_dataset(dataset)\n",
    "        dataset_tokens[ds] = tokenize_corpus(list(df['text']))\n",
    "    dataset_features[ds] = get_features(dataset_tokens[ds], min_df=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13974b",
   "metadata": {},
   "source": [
    "Compare dataset features with an external [English vocabulary](https://github.com/dwyl/english-words) used as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114085b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"words_dictionary.json\", \"r\") as vocab_file:\n",
    "    eng_vocab = set(json.loads(vocab_file.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bea3d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stats = {}\n",
    "for i, dataset in enumerate(dataset_features):\n",
    "    vocab_size = len(dataset_features[dataset])\n",
    "    eng_words = len(eng_vocab.intersection(dataset_features[dataset]))\n",
    "    ratio = int(eng_words*100 / vocab_size)\n",
    "    features_stats[i] = [dataset, vocab_size, eng_words, ratio]\n",
    "\n",
    "df_features_stats = pd.DataFrame.from_dict(features_stats, \n",
    "                                           orient='index',\n",
    "                                           columns=[\"source\", \"vocabulary size\", \"english words\", \"eng%\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80332b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d433d5",
   "metadata": {},
   "source": [
    "Compare features of synthetic datasets (GPT-2) vs human-written dataset (WebText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16559a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_machine_features_stats = []\n",
    "ref_human_features = dataset_features['webtext.train']\n",
    "\n",
    "for dataset in dataset_features:\n",
    "    if ('GPT2' in dataset and 'webtext' not in dataset):\n",
    "        feat_union_size = len(dataset_features[dataset].union(ref_human_features))\n",
    "        feat_intersection_size = len(dataset_features[dataset].intersection(ref_human_features))\n",
    "        ratio = int(feat_intersection_size*100 / feat_union_size)\n",
    "        human_machine_features_stats.append([\n",
    "            f'{dataset} VS WebText', \n",
    "            feat_union_size,\n",
    "            feat_intersection_size,\n",
    "            ratio])\n",
    "\n",
    "df_human_machine_features_stats = pd.DataFrame(human_machine_features_stats,\n",
    "                                               columns=['source', 'combined vocab', 'shared vocab', 'shared ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8533ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human_machine_features_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16453b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gtd-env]",
   "language": "python",
   "name": "conda-env-gtd-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
