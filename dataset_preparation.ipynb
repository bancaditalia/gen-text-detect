{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5901c4eb",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dca916",
   "metadata": {},
   "source": [
    "This notebook prepares the original datasets for training and testing the text classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d279b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593776d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73d56b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download datasets\n",
    "%run 'download_dataset.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e118be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data\"\n",
    "datasets = sorted([f for f in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, f)) and f[0] != \".\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b9c6c",
   "metadata": {},
   "source": [
    "### Load dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb20ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset, data_path=\"./data\"):\n",
    "    df = None\n",
    "    path = os.path.join(data_path, dataset)\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_json(path, lines = True)\n",
    "        # adjust DataFrame based on original data structure\n",
    "        if len(df.columns)==1:\n",
    "            # json contains only texts (GPT3 samples)\n",
    "            df.rename(columns={0: 'text'}, inplace=True)\n",
    "        elif 'article' in df.columns:\n",
    "            # Grover datasets\n",
    "            df.rename(columns={'article': 'text'}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d942bce",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0345f31e",
   "metadata": {},
   "source": [
    "Tokenization and filtering function to preprocess datasets (removing digits-only tokens and non-english symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ed567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing as pp\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def tokenize_corpus(corpus):\n",
    "    tokenized_corpus = make_pipeline(\n",
    "        pp.WordTokenizer(), \n",
    "        pp.WordsFilter(drop_symbols=True, drop_digits=True)\n",
    "    ).fit_transform(corpus)\n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9a80a1",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f96572",
   "metadata": {},
   "source": [
    "Select higher quality samples from the training datasets by using an external [English vocabulary](https://github.com/dwyl/english-words) to evaluate ratio of english words in the training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0612a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./data/training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8dc024",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datasets = ['webtext.train.jsonl', 'GPT2-xl-1542M.train.jsonl', 'GPT2-xl-1542M-k40.train.jsonl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda26ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"words_dictionary.json\", \"r\") as vocab_file:\n",
    "    eng_vocab = set(json.loads(vocab_file.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4bb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in training_datasets:\n",
    "    df = load_dataset(ds)\n",
    "    eng_vocab_overlap = []\n",
    "    eng_ratio = []\n",
    "    for doc_tokens in tokenize_corpus(df['text'].to_list()):\n",
    "        count = 0\n",
    "        for token in doc_tokens:\n",
    "            if token in eng_vocab:\n",
    "                count += 1\n",
    "        eng_vocab_overlap.append(count)\n",
    "        if len(doc_tokens) == 0:\n",
    "            eng_ratio.append(0)\n",
    "        else:\n",
    "            eng_ratio.append(int(count*100/len(doc_tokens)))\n",
    "    df = df[['id', 'text']]\n",
    "    df[\"english words\"] = eng_vocab_overlap\n",
    "    df[\"english %\"] = eng_ratio\n",
    "    df_filtered = df[df[\"english %\"]>=90]\n",
    "    df_filtered = df_filtered[df_filtered[\"english words\"]>20]\n",
    "    df_filtered.sample(n=200000, random_state=seed).to_json(\n",
    "        os.path.join(output_path, f'{os.path.splitext(ds)[0]}.filtered.jsonl'), \n",
    "        orient=\"records\", \n",
    "        lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bdc39c",
   "metadata": {},
   "source": [
    "## Prepare test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf79548",
   "metadata": {},
   "source": [
    "Reorganize test data separating \"machine\" vs \"human\" texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb47d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d74a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in [d for d in datasets if 'test' in d]:\n",
    "    path = os.path.join(data_path, ds)\n",
    "    if os.path.exists(path):\n",
    "        name, ext = os.path.splitext(ds)\n",
    "        df = load_dataset(ds)\n",
    "        df['id'] = df.index\n",
    "        if 'GPT' in ds:\n",
    "            df[['id', 'text']].to_json(os.path.join(output_path, f'{name}.machine.jsonl'), orient=\"records\", lines=True)\n",
    "            #shutil.copy(path, os.path.join(output_path, f\"{name}.machine{ext}\"))\n",
    "        elif 'webtext' in ds:\n",
    "            df[['id', 'text']].to_json(os.path.join(output_path, f'{name}.human.jsonl'), orient=\"records\", lines=True)\n",
    "            #shutil.copy(path, os.path.join(output_path, f\"{name}.machine{ext}\"))\n",
    "        elif 'Grover' in ds:\n",
    "            # need to unpack datasets to differentiate machine/human sources\n",
    "            human_texts = df[df.label == \"human\"]\n",
    "            machine_texts = df[df.label == \"machine\"]\n",
    "            # Grover human samples are the same regardless of generator-size\n",
    "            grover_human_path = os.path.join(output_path, f'Grover.human.jsonl')\n",
    "            if not os.path.exists(grover_human_path):\n",
    "                human_texts[['id', 'text']].to_json(grover_human_path, orient=\"records\", lines=True)\n",
    "            machine_texts[['id', 'text']].to_json(os.path.join(output_path, f'{name}.machine.jsonl'), orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79c099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gtd-env]",
   "language": "python",
   "name": "conda-env-gtd-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
